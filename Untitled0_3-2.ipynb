{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0-3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "BD5BeVZyBZce",
        "KkfG6af_Irvp",
        "MfwpWZakJL-f",
        "q495b2B2Jaog",
        "7l_JyvgiJopU"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Model\n",
        "\n"
      ],
      "metadata": {
        "id": "BD5BeVZyBZce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################################\n",
        "# Imports\n",
        "#######################################################################\n",
        "import logging\n",
        "#This is going to help log information throughout our program. \n",
        "logging.basicConfig(\n",
        "        level=logging.INFO, format=\"%(asctime)s %(levelname)s:%(message)s\"\n",
        "    )\n",
        "logging.info(\"Loading libraries, takes aproximately 4 seconds.\")\n",
        "\n",
        "\n",
        "# Libraries that will be needed for everything to run\n",
        "import cv2 as cv # used for handling the image aspect\n",
        "import tarfile # used to untar the model downloaded\n",
        "import shutil\n",
        "import urllib.request # used to download\n",
        "import os # file handling\n",
        "import tensorflow as tf # used for detections model\n",
        "import numpy as np # V1.2, added to fix adv noise saving.\n",
        "import urllib\n",
        "import urllib.request\n",
        "logging.info('Done.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUZSmeXZBhmT",
        "outputId": "8b70acab-5a6b-4080-ba8e-5e54691e2574"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-07-21 08:30:50,315 INFO:Loading libraries, takes aproximately 4 seconds.\n",
            "2022-07-21 08:30:58,222 INFO:NumExpr defaulting to 2 threads.\n",
            "2022-07-21 08:30:59,024 INFO:Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "\n"
      ],
      "metadata": {
        "id": "KkfG6af_Irvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Model():\n",
        "  def __init__(self):\n",
        "\n",
        "    self.download_model()\n",
        "    self.model = self.initialize_model()\n",
        "\n",
        "#######################################################################\n",
        "#\n",
        "#     Function that downloads models,\n",
        "#     Input: Base Url, File name Outputs: None. \n",
        "#     Purpose:Downloads files\n",
        "#\n",
        "#######################################################################\n",
        "  def Download(self,base_url, file_name):\n",
        "    # checks if file exists\n",
        "\n",
        "    if file_name not in os.listdir():\n",
        "    \n",
        "      #logs what is about to happen\n",
        "      logging.info('Downloading ' + file_name) \n",
        "      \n",
        "      #builds url\n",
        "      url = base_url + file_name\n",
        "      \n",
        "      #retrieves what ever was in url by downloading it ...\n",
        "      urllib.request.urlretrieve(url, file_name)\n",
        "      logging.info('Download Complete')\n",
        "      \n",
        "      #This part checks it is is a tar file, a tar file is bunched up. \n",
        "      if file_name.find('.tar.gz') != -1:\n",
        "        logging.info(\"Extracting \" + file_name)\n",
        "      \n",
        "      # we are naming the the directory\n",
        "      dir_name = file_name[0:-len('.tar.gz')]\n",
        "      \n",
        "      #check to see if path exist.\n",
        "      if os.path.exists(dir_name):\n",
        "\n",
        "      #if it does we want to erase the entire tree using shutil.rmtree\n",
        "        shutil.rmtree(dir_name) \n",
        "      \n",
        "      # Now we want to open the tarfile and extract all the information \n",
        "      tarfile.open(file_name, 'r:gz').extractall('./')\n",
        "      \n",
        "      #log extraction complete \n",
        "      logging.info(\"Extraction Complete\")\n",
        "    \n",
        "    \n",
        "    #if the file exist then we can  \n",
        "    else:\n",
        "      logging.info(file_name + ' already exists.')\n",
        "\n",
        "#######################################################################\n",
        "#\n",
        "#   download_model  Inputs: None    Outputs: None     \n",
        "#   \n",
        "#######################################################################\n",
        "  \n",
        "  def download_model(self):\n",
        "    #makes a call to download label.txt\n",
        "    self.Download('https://raw.githubusercontent.com/nightrome/cocostuff/master/','labels.txt')\n",
        "    #makes a call to download model\n",
        "    self.Download('http://download.tensorflow.org/models/object_detection/', 'ssd_mobilenet_v1_coco_2018_01_28.tar.gz')\n",
        "\n",
        "#######################################################################\n",
        "\n",
        "#######################################################################\n",
        "# initialize_model    \n",
        "# Inputs: None      \n",
        "# Outputs: Returns a model that accepts one input. That model accepts tensored images... it then returns the detections  \n",
        "#\n",
        "#######################################################################  \n",
        "  def initialize_model(self):\n",
        "    #this is a preprocessing function need to help with the frozen graph\n",
        "    def wrap_graph(graph_def, inputs, outputs, print_graph=False):\n",
        "      wrapped = tf.compat.v1.wrap_function(lambda: tf.compat.v1.import_graph_def(graph_def, name=\"\"), [])\n",
        "      \n",
        "      return wrapped.prune(\n",
        "          tf.nest.map_structure(wrapped.graph.as_graph_element, inputs),\n",
        "          tf.nest.map_structure(wrapped.graph.as_graph_element, outputs))\n",
        "    \n",
        "    \n",
        "    # This is the format that we want outputs to follow  \n",
        "    outputs = (\n",
        "      'num_detections:0',\n",
        "     'detection_classes:0',\n",
        "     'detection_scores:0',\n",
        "     'detection_boxes:0',\n",
        "    )\n",
        "\n",
        "    \n",
        "    #We need to load a pre-trained model... This is done with the following. We care about frozen_inference_graph.pb because it holds a trained Tensor Flow graph that we can use for classification.\n",
        "    frozen_graph = os.path.join('ssd_mobilenet_v1_coco_2018_01_28', 'frozen_inference_graph.pb')\n",
        "    \n",
        "    #This part of the code allows us to read the frozen graph (model). As you can see it saves and loads the file. the first part instantiates and the second part used parsefromstring in order to \n",
        "    #this part allows us to go through the file\n",
        "    with tf.io.gfile.GFile(frozen_graph, \"rb\") as f:\n",
        "      \n",
        "      #This initializes a GraphDef which will be used to help with the loading part\n",
        "      graph_def = tf.compat.v1.GraphDef()\n",
        "      \n",
        "      #This reads the files that we have\n",
        "      loaded = graph_def.ParseFromString(f.read())\n",
        "\n",
        "      #This used the frozen graph part and this makes the model. \n",
        "      model = wrap_graph(graph_def=graph_def, inputs=[\"image_tensor:0\"], outputs=outputs)\n",
        "      return model\n",
        "\n",
        "#######################################################################"
      ],
      "metadata": {
        "id": "RxkvcJiZIzcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize Model\n"
      ],
      "metadata": {
        "id": "MfwpWZakJL-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################################\n",
        "#     We need a way to initialize the model and the name it something in order to pass it into the image class\n",
        "#     Once we pass it into the image class we can then get the num of detections and so forth.\n",
        "#######################################################################\n",
        "\n",
        "model= Model().initialize_model()"
      ],
      "metadata": {
        "id": "fbl7GTEXJNYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Class\n"
      ],
      "metadata": {
        "id": "-YLtcjtUJDhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make model at beginning...\n",
        "#######################################################################\n",
        "\n",
        "class Image():\n",
        "  priv_class=[12, 14, 30, 33, 46, 63, 65, 66, 70, 71, 72, 73, 74, 75, 76, 77, 85, 110, 133,31,69,3]\n",
        "  \n",
        "#######################################################################\n",
        "#   Purpose: \n",
        "#   Gets image from what is passed in \n",
        "#   Input: \n",
        "#   image_path or file name\n",
        "#   Output:\n",
        "#   image downloaded or image\n",
        "#######################################################################\n",
        "\n",
        "  def get_image(self, image_path):\n",
        "    print(\"1\")\n",
        "    if image_path in os.listdir():\n",
        "      print(\"2\")\n",
        "      image = cv.imread(image_path)\n",
        "      image=cv.cvtColor(image,cv.COLOR_BGR2RGB)\n",
        "    else:\n",
        "      print(\"3\")\n",
        "      req = urllib.request.urlopen(image_path)\n",
        "      arr = np.asarray(bytearray(req.read()), dtype=np.uint8)\n",
        "      image = cv.imdecode(arr, -1)\n",
        "    \n",
        "    return image\n",
        "\n",
        "#######################################################################\n",
        "#       VIPER Helpers\n",
        "#######################################################################\n",
        "\n",
        "\n",
        "#######################################################################\n",
        "#   Purpose: \n",
        "#   Used for a loop in Viper_blur to verify that our boxed detection is in our priv_class list \n",
        "#   Input: \n",
        "#   box_class (The detection class that our individual box is in)\n",
        "#   Output:\n",
        "#   Boolean\n",
        "#######################################################################\n",
        "  def VIPER_COCO_priv_or_not(self,box_class):\n",
        "      \n",
        "    if box_class in self.priv_class:\n",
        "      return True      \n",
        "    else:\n",
        "      return False\n",
        "\n",
        "#######################################################################\n",
        "#   Purpose:\n",
        "#   Used for a loop in Viper_blur to Blur an object \n",
        "#   \n",
        "#   We call on this function repeatedly for every detection box in our detection.\n",
        "#######################################################################\n",
        "  def VIPER_object_blur(self, cv2_image, x1, x2, y1, y2, blurAmount):\n",
        "        \n",
        "        #defines the outline of what we want to blur\n",
        "    blurred_object = cv2_image[y1:y2,x1:x2]\n",
        "        \n",
        "        #This is the amount we want to blur \n",
        "    ksize = (blurAmount,blurAmount)\n",
        "      \n",
        "        #We transform what we have in or outline into a blurred object  \n",
        "    blurred_object = cv.blur(blurred_object, ksize, cv.BORDER_DEFAULT)\n",
        "        \n",
        "        #We modify the section of the image to our blurred out version.\n",
        "    cv2_image[y1:y2,x1:x2] = blurred_object\n",
        "\n",
        "        #Returns the image\n",
        "    \n",
        "    self.modified= cv2_image\n",
        "    self.modified=cv.cvtColor(self.modified,cv.COLOR_BGR2RGB)\n",
        "    cv.imwrite('image.jpg',self.modified)\n",
        "    return self.modified\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#######################################################################\n",
        "  def image_saver(ImageType):\n",
        "    image=ImageType.image\n",
        "    image=cv.cvtColor(image,cv.COLOR_BGR2RGB)\n",
        "    cv.imwrite('image.jpg',image)\n",
        "#######################################################################\n",
        "#######################################################################\n",
        "#     This function takes in box bound and an image, then it turn it into a coordinate\n",
        "#     This allows us to border out what we want to pixalize. \n",
        "#######################################################################\n",
        "    \n",
        "\n",
        "  def VIPER_COCO_box_to_pixels(self, box_bounds, cv2_image):\n",
        "        \n",
        "    y1 = int((box_bounds[0]*cv2_image.shape[0]).numpy())\n",
        "    x1 = int((box_bounds[1]*cv2_image.shape[1]).numpy())\n",
        "    y2 = int((box_bounds[2]*cv2_image.shape[0]).numpy())\n",
        "    x2 = int((box_bounds[3]*cv2_image.shape[1]).numpy())\n",
        "    \n",
        "    return [x1,x2,y1,y2]\n",
        "\n",
        "\n",
        "#######################################################################\n",
        "\n",
        "#######################################################################\n",
        "    \n",
        "  def VIPER_object_block(cv2_image, x1, x2, y1, y2, color):\n",
        "    thickness = -1\n",
        "    cv.rectangle(cv2_image,(x1,y1),(x2,y2), color, thickness)\n",
        "\n",
        "    return cv2_image\n",
        "\n",
        "#######################################################################        \n",
        "#######################################################################\n",
        "#    def VIPER_block(self):\n",
        "#      for i in range(self.num_of_detections):\n",
        "#        box = self.detections[3][0][i]\n",
        "#        box_class = self.detections[1][0][i]\n",
        "#        color = (0, 0, 0)\n",
        "#        #\n",
        "#        if VIPER_COCO_priv_or_not(box_class):\n",
        "#          left, right, top, bottom = VIPER_COCO_box_to_pixels(box, ImageType.image)\n",
        "#          VIPER_object_block(ImageType.image, left, right, top, bottom, color)\n",
        "#######################################################################\n",
        "#   This function will also be called in a loop in order \n",
        "#######################################################################\n",
        "  def VIPER_COCO_priv_or_not(self,box_class):\n",
        "      \n",
        "    if box_class in self.priv_class:\n",
        "      return True      \n",
        "    else:\n",
        "      return False\n",
        "\n",
        "#######################################################################\n",
        "#######################################################################\n",
        "    \n",
        "  def VIPER_blur(self):\n",
        "      \n",
        "    for i in range(self.num_of_detections):\n",
        "        \n",
        "      single_box = self.detections[3][0][i]\n",
        "      box_class = self.detections[1][0][i]\n",
        "      \n",
        "      if self.VIPER_COCO_priv_or_not(box_class): \n",
        "        \n",
        "        #VIPER_object_block(self.image, left, right, top, bottom, color)\n",
        "        left, right, top, bottom = self.VIPER_COCO_box_to_pixels(single_box, self.image)\n",
        "        self.VIPER_object_blur(self.image,left, right, top, bottom, 300)\n",
        "\n",
        "#######################################################################\n",
        "#######################################################################\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Image Handlers\n",
        "#######################################################################\n",
        "\n",
        "# image changer\n",
        "  def image_modifier(ImageType, modifier):\n",
        "      if modifier.lower() == \"blur\" or modifier.lower()  == 'br':\n",
        "        self.VIPER_blur()\n",
        "      #elif modifier.lower()  == \"block\" or modifier.lower()  == 'bk':\n",
        "      #  VIPER_block(ImageType)\n",
        "      #elif modifier.lower()  == \"noise\" or modifier.lower()  == 'ne':\n",
        "      #  VIPER_advarserial_modifier(ImageType)\n",
        "      #elif modifier.lower()  == \"saliency\" or modifier.lower()  == 'sy':\n",
        "      #  VIPER_saliency_modifier(ImageType)\n",
        "      #else:\n",
        "      #  logging.info(\"No modifier ran.\")\n",
        "\n",
        "#######################################################################\n",
        "#######################################################################\n",
        "\n",
        "\n",
        "  # add model as parameter\n",
        "  def __init__(self, image_path, model):\n",
        "    self.image= self.get_image(image_path)\n",
        "    self.tensor = tf.convert_to_tensor([self.image], dtype=tf.uint8)\n",
        "    self.model= model\n",
        "    self.detections = self.model(self.tensor)\n",
        "    print(self.detections[0][0])\n",
        " \n",
        "    \n",
        "    self.num_of_detections= len([x for x in self.detections[3][0] if not ((x == self.detections[2][0][-1])[0])])\n",
        "    self.detection_classes= [self.detections[1][0][x].numpy() for x in range(self.num_of_detections)]\n",
        "    self.boxed_detections= self.detections[3][0]\n",
        "    self.modified=self.VIPER_blur\n",
        "    self.object_classes={}\n",
        "    \n",
        "#######################################################################      \n",
        "    for line in open(\"labels.txt\"):\n",
        "      line = line[:-1]\n",
        "      key, value = line.split(': ')\n",
        "      self.object_classes[int(key)] = value\n",
        "    for i in self.detection_classes:\n",
        "      object_class = int(i)\n",
        "      object_bool = self.VIPER_COCO_priv_or_not(object_class)\n",
        "#######################################################################\n",
        "\n"
      ],
      "metadata": {
        "id": "Bn3Nl2J7kb9Q"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize Image Class... pass in image and model \n"
      ],
      "metadata": {
        "id": "q495b2B2Jaog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "desk=Image(\"image.jpg\",model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWnBPqlKPlYz",
        "outputId": "696c2fdd-0605-4f0a-f74f-a1fbb36bb8cc"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "tf.Tensor(2.0, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "desk.VIPER_blur()\n"
      ],
      "metadata": {
        "id": "JJYLzSRP80tG"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "desk.modified"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0mmZ4TnCujP",
        "outputId": "8e315819-8e96-45ef-9c4b-15b06d67524f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[164, 176, 178],\n",
              "        [164, 176, 178],\n",
              "        [164, 176, 178],\n",
              "        ...,\n",
              "        [216, 224, 223],\n",
              "        [216, 224, 223],\n",
              "        [216, 224, 223]],\n",
              "\n",
              "       [[164, 176, 178],\n",
              "        [164, 176, 178],\n",
              "        [164, 176, 178],\n",
              "        ...,\n",
              "        [216, 224, 223],\n",
              "        [216, 224, 223],\n",
              "        [216, 224, 223]],\n",
              "\n",
              "       [[163, 175, 177],\n",
              "        [163, 175, 177],\n",
              "        [163, 175, 177],\n",
              "        ...,\n",
              "        [216, 224, 223],\n",
              "        [216, 224, 223],\n",
              "        [216, 224, 223]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[190, 176, 170],\n",
              "        [189, 175, 169],\n",
              "        [188, 174, 168],\n",
              "        ...,\n",
              "        [204, 199, 198],\n",
              "        [202, 197, 196],\n",
              "        [201, 196, 195]],\n",
              "\n",
              "       [[193, 179, 173],\n",
              "        [190, 176, 170],\n",
              "        [187, 173, 167],\n",
              "        ...,\n",
              "        [202, 197, 196],\n",
              "        [199, 194, 193],\n",
              "        [197, 192, 191]],\n",
              "\n",
              "       [[193, 179, 173],\n",
              "        [190, 176, 170],\n",
              "        [187, 173, 167],\n",
              "        ...,\n",
              "        [202, 197, 196],\n",
              "        [199, 194, 193],\n",
              "        [197, 192, 191]]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Needs implementation\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7l_JyvgiJopU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# saliency\n",
        "    def VIPER_saliency_modifier(ImageType):\n",
        "      saliency = cv.saliency.StaticSaliencySpectralResidual_create() #This intializes the saliency, and the below code creates a finer image which possibly could be used for instance segmentation\n",
        "\n",
        "      (success, saliencyMap) = saliency.computeSaliency(ImageType.image) # this computes the map\n",
        "      ImageType.image = (saliencyMap * 255).astype(\"uint8\") # type casts and standardizes the image pixels \n",
        "  # threshMap = cv.threshold(saliencyMap, 0, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)[1] #applies thereshold to get binary image from map \n",
        "\n",
        "# adverserial noise - FIX FIX FIX FIX\n",
        "    def VIPER_advarserial_modifier(ImageType):\n",
        "  # Helper function to preprocess the image so that it can be inputted in MobileNetV2\n",
        "      def preprocess(image):\n",
        "        image = tf.cast(image, tf.float32)\n",
        "        image = tf.image.resize(image, (224, 224))\n",
        "        image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n",
        "        image = image[None, ...]\n",
        "        return image\n",
        "\n",
        "      def create_adversarial_pattern(input_image, input_label, pretrained_model):\n",
        "        loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
        "        with tf.GradientTape() as tape:\n",
        "          tape.watch(input_image)\n",
        "          prediction = pretrained_model(input_image)\n",
        "          loss = loss_object(input_label, prediction)\n",
        "\n",
        "        gradient = tape.gradient(loss, input_image) # Get the gradients of the loss w.r.t to the input image.\n",
        "        signed_grad = tf.sign(gradient) # Get the sign of the gradients to create the perturbation\n",
        "\n",
        "        return signed_grad\n",
        "\n",
        "      pretrained_model = tf.keras.applications.MobileNetV2(include_top=True,weights='imagenet')\n",
        "      pretrained_model.trainable = False\n",
        "\n",
        "      adv_image = preprocess(ImageType.image)\n",
        "      image_probs = pretrained_model.predict(adv_image)\n",
        "\n",
        "  # Get the input label of the image.\n",
        "      labrador_retriever_index = 1\n",
        "      label = tf.one_hot(labrador_retriever_index, image_probs.shape[-1])\n",
        "      label = tf.reshape(label, (1, image_probs.shape[-1]))\n",
        "\n",
        "      perturbations = create_adversarial_pattern(adv_image, label, pretrained_model)\n",
        "\n",
        "      eps = 0.05\n",
        "      adv_x = adv_image + eps*perturbations\n",
        "      adv_x = tf.clip_by_value(adv_x, -1, 1)\n",
        "      ImageType.image = adv_x[0]\n",
        "      ImageType.image = np.clip(ImageType.image, 0, 1)\n",
        "      ImageType.image = (ImageType.image * 255).astype(np.uint8)"
      ],
      "metadata": {
        "id": "lK63sjNQJm7Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}